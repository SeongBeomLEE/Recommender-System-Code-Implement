{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_Rec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CWM6iuXqhrztLmn8e99bR1V7D-q0cM6t",
      "authorship_tag": "ABX9TyOspG6dYxGPYXXGPbi2Uc+3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDOlz7QpRse_"
      },
      "source": [
        "https://github.com/younggyoseo/vae-cf-pytorch\n",
        "\n",
        "https://github.com/dawenl/vae_cf\n",
        "\n",
        "학습이 잘 안되는 것 같음 이유가 무엇인지 알아봐야 할 듯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts83iBYUrvX2"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sn\n",
        "sn.set()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import bottleneck as bn\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYKff6cn0_aE"
      },
      "source": [
        "# Data 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbttiOWvuNb"
      },
      "source": [
        "MovieLens 20M Dataset의 평점 데이터만 사용 (Implicit Feedback)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3ptJh-Z1OuQ"
      },
      "source": [
        "class DataLoader():\n",
        "    '''\n",
        "    Load Movielens-20m dataset\n",
        "    '''\n",
        "    def __init__(self, path):\n",
        "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
        "        assert os.path.exists(self.pro_dir), \"Preprocessed files does not exist. Run data.py\"\n",
        "\n",
        "        self.n_items = self.load_n_items()\n",
        "    \n",
        "    def load_data(self, datatype='train'):\n",
        "        if datatype == 'train':\n",
        "            return self._load_train_data()\n",
        "        elif datatype == 'validation':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        elif datatype == 'test':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        else:\n",
        "            raise ValueError(\"datatype should be in [train, validation, test]\")\n",
        "        \n",
        "    def load_n_items(self):\n",
        "        unique_sid = list()\n",
        "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                unique_sid.append(line.strip())\n",
        "        n_items = len(unique_sid)\n",
        "        return n_items\n",
        "    \n",
        "    def _load_train_data(self):\n",
        "        path = os.path.join(self.pro_dir, 'train.csv')\n",
        "        \n",
        "        tp = pd.read_csv(path)\n",
        "        n_users = tp['uid'].max() + 1\n",
        "\n",
        "        rows, cols = tp['uid'], tp['sid']\n",
        "        data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                                 (rows, cols)), dtype='float64',\n",
        "                                 shape=(n_users, self.n_items))\n",
        "        return data\n",
        "    \n",
        "    def _load_tr_te_data(self, datatype='test'):\n",
        "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
        "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
        "\n",
        "        tp_tr = pd.read_csv(tr_path)\n",
        "        tp_te = pd.read_csv(te_path)\n",
        "\n",
        "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        return data_tr, data_te"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJNSBG3i1Pr4"
      },
      "source": [
        "def get_count(tp, id):\n",
        "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
        "    count = playcount_groupbyid.size()\n",
        "    return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_kDO2zo1Q2o"
      },
      "source": [
        "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "    if min_sc > 0:\n",
        "        itemcount = get_count(tp, 'movieId')\n",
        "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount['size'] >= min_sc])]\n",
        "    \n",
        "    if min_uc > 0:\n",
        "        usercount = get_count(tp, 'userId')\n",
        "        print(usercount)\n",
        "        tp = tp[tp['userId'].isin(usercount.index[usercount['size'] >= min_uc])]\n",
        "    \n",
        "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
        "    return tp, usercount, itemcount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r18-mZLX1SRA"
      },
      "source": [
        "def split_train_test_proportion(data, test_prop=0.2):\n",
        "    data_grouped_by_user = data.groupby('userId')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "\n",
        "    for _, group in data_grouped_by_user:\n",
        "        n_items_u = len(group)\n",
        "\n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool')\n",
        "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "\n",
        "            tr_list.append(group[np.logical_not(idx)])\n",
        "            te_list.append(group[idx])\n",
        "        \n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "        \n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "\n",
        "    return data_tr, data_te"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwGq4Rh_1UCs"
      },
      "source": [
        "def numerize(tp, profile2id, show2id):\n",
        "    uid = tp['userId'].apply(lambda x: profile2id[x])\n",
        "    sid = tp['movieId'].apply(lambda x: show2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "3n3Gc-2S1agf",
        "outputId": "f1deba95-d6d9-47e9-d7ec-bfe912c88f0d"
      },
      "source": [
        "DATA_DIR = '/content/drive/MyDrive/투빅스 컨퍼런스/'\n",
        "\n",
        "raw_data = pd.read_csv(DATA_DIR + 'ratings.csv')\n",
        "raw_data = raw_data[raw_data['rating'] > 3.5]\n",
        "raw_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1094785734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>223</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1112485573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>253</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1112484940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>260</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1112484826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>293</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1112484703</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    userId  movieId  rating   timestamp\n",
              "6        1      151     4.0  1094785734\n",
              "7        1      223     4.0  1112485573\n",
              "8        1      253     4.0  1112484940\n",
              "9        1      260     4.0  1112484826\n",
              "10       1      293     4.0  1112484703"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmqoGY7P1wF3",
        "outputId": "1c30e442-a3ec-44c7-d1a9-33708bd7b4fd"
      },
      "source": [
        "raw_data, user_activity, item_popularity = filter_triplets(raw_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        userId  size\n",
            "0            1    88\n",
            "1            2    43\n",
            "2            3   145\n",
            "3            4    16\n",
            "4            5    50\n",
            "...        ...   ...\n",
            "138282  138489    27\n",
            "138283  138490    86\n",
            "138284  138491     5\n",
            "138285  138492    61\n",
            "138286  138493   301\n",
            "\n",
            "[138287 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "zleXmCRD1yao",
        "outputId": "9a069b6b-9f93-43d9-a6fa-3aa640430782"
      },
      "source": [
        "raw_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>151</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1094785734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>223</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1112485573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>253</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1112484940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>260</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1112484826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>293</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1112484703</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    userId  movieId  rating   timestamp\n",
              "6        1      151     4.0  1094785734\n",
              "7        1      223     4.0  1112485573\n",
              "8        1      253     4.0  1112484940\n",
              "9        1      260     4.0  1112484826\n",
              "10       1      293     4.0  1112484703"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-g0aHp5V1zdY",
        "outputId": "92a70a9b-9127-4693-c965-a90d9dc85b73"
      },
      "source": [
        "user_activity.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userId  size\n",
              "0       1    88\n",
              "1       2    43\n",
              "2       3   145\n",
              "3       4    16\n",
              "4       5    50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfGEFdEs1zoo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4c15cab3-b82a-46ee-ea58-09244c86dbfd"
      },
      "source": [
        "item_popularity.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>32831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>7196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>3245</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   movieId   size\n",
              "0        1  32831\n",
              "1        2   7196\n",
              "2        3   3952\n",
              "3        4    687\n",
              "4        5   3245"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LyIfbiu4zAa"
      },
      "source": [
        "# Shuffle User Indices\n",
        "unique_uid = user_activity.index\n",
        "np.random.seed(98765)\n",
        "idx_perm = np.random.permutation(unique_uid.size)\n",
        "unique_uid = unique_uid[idx_perm]\n",
        "\n",
        "n_users = unique_uid.size\n",
        "n_heldout_users = 10000\n",
        "\n",
        "# Split Train/Validation/Test User Indices\n",
        "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "te_users = unique_uid[(n_users - n_heldout_users):]\n",
        "\n",
        "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
        "unique_sid = pd.unique(train_plays['movieId'])\n",
        "\n",
        "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
        "\n",
        "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
        "unique_sid = pd.unique(train_plays['movieId'])\n",
        "\n",
        "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
        "\n",
        "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "\n",
        "if not os.path.exists(pro_dir):\n",
        "    os.makedirs(pro_dir)\n",
        "\n",
        "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "    for sid in unique_sid:\n",
        "        f.write('%s\\n' % sid)\n",
        "\n",
        "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
        "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]\n",
        "\n",
        "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
        "\n",
        "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
        "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]\n",
        "\n",
        "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
        "\n",
        "train_data = numerize(train_plays, profile2id, show2id)\n",
        "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
        "\n",
        "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
        "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
        "\n",
        "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
        "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
        "\n",
        "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
        "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
        "\n",
        "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
        "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5IMdPi71C3D"
      },
      "source": [
        "# Model 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cDMVvxM5UAx"
      },
      "source": [
        "class MultiDAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-DAE.\n",
        "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
        "        super(MultiDAE, self).__init__()\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "\n",
        "        self.dims = self.q_dims + self.p_dims[1:]\n",
        "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "        self.init_weights()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        h = F.normalize(input)\n",
        "        h = self.drop(h)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.weights) - 1:\n",
        "                h = F.tanh(h)\n",
        "        return h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFudoYhI5Y6j"
      },
      "source": [
        "class MultiVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-VAE.\n",
        "    Multi-VAE : Variational Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
        "        super(MultiVAE, self).__init__()\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "\n",
        "        # Last dimension of q- network is for mean and variance\n",
        "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
        "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
        "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
        "        \n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.init_weights()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        mu, logvar = self.encode(input)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "    \n",
        "    def encode(self, input):\n",
        "        h = F.normalize(input)\n",
        "        h = self.drop(h)\n",
        "        \n",
        "        for i, layer in enumerate(self.q_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.q_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "            else:\n",
        "                mu = h[:, :self.q_dims[-1]]\n",
        "                logvar = h[:, self.q_dims[-1]:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "    \n",
        "    def decode(self, z):\n",
        "        h = z\n",
        "        for i, layer in enumerate(self.p_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.p_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "        return h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.q_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "        \n",
        "        for layer in self.p_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR7dVOXz5cab"
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar, anneal=1.0):\n",
        "    # BCE = F.binary_cross_entropy(recon_x, x)\n",
        "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
        "\n",
        "    return BCE + anneal * KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbOapt3V1FHq"
      },
      "source": [
        "# Metric 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgptZDVo0hZj"
      },
      "source": [
        "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)])\n",
        "    return DCG / IDCG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbFJ7kGL5hxA"
      },
      "source": [
        "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
        "    return recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXD5EeA5msj"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiAurpFn67Su",
        "outputId": "5030ff67-81b5-4cd6-e2a9-95750e1b187e"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3RC6R0I7-sl"
      },
      "source": [
        "lr = 1e-4\n",
        "wd = 0.00\n",
        "batch_size = 500\n",
        "epochs = 200\n",
        "total_anneal_steps = 200000\n",
        "anneal_cap = 0.2\n",
        "log_interval = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB6S3svG5mGL"
      },
      "source": [
        "loader = DataLoader(DATA_DIR)\n",
        "\n",
        "n_items = loader.load_n_items()\n",
        "train_data = loader.load_data('train')\n",
        "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "N = train_data.shape[0]\n",
        "idxlist = list(range(N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLbWoUiQ65i5"
      },
      "source": [
        "p_dims = [200, 600, n_items]\n",
        "model = MultiVAE(p_dims).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "criterion = loss_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_ALpgUW7sqU"
      },
      "source": [
        "def sparse2torch_sparse(data):\n",
        "    \"\"\"\n",
        "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
        "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
        "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
        "    \"\"\"\n",
        "    samples = data.shape[0]\n",
        "    features = data.shape[1]\n",
        "    coo_data = data.tocoo()\n",
        "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
        "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
        "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
        "    values = np.array([row2val[r] for r in coo_data.row])\n",
        "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8KQKxfJ8vRo"
      },
      "source": [
        "def naive_sparse2tensor(data):\n",
        "    return torch.FloatTensor(data.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jng-ndBb8wjy"
      },
      "source": [
        "def train():\n",
        "    # Turn on training mode\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    start_time = time.time()\n",
        "    global update_count\n",
        "\n",
        "    np.random.shuffle(idxlist)\n",
        "    \n",
        "    for batch_idx, start_idx in enumerate(range(0, N, batch_size)):\n",
        "        end_idx = min(start_idx + batch_size, N)\n",
        "        data = train_data[idxlist[start_idx:end_idx]]\n",
        "        data = naive_sparse2tensor(data).to(device)\n",
        "\n",
        "        if total_anneal_steps > 0:\n",
        "            anneal = min(anneal_cap, \n",
        "                            1. * update_count / total_anneal_steps)\n",
        "        else:\n",
        "            anneal = anneal_cap\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        \n",
        "        loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        update_count += 1\n",
        "\n",
        "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
        "                    'loss {:4.2f}'.format(\n",
        "                        epoch, batch_idx, len(range(0, N, batch_size)),\n",
        "                        elapsed * 1000 / log_interval,\n",
        "                        train_loss / log_interval))\n",
        "\n",
        "            start_time = time.time()\n",
        "            train_loss = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS4ACjAj9XbW"
      },
      "source": [
        "def evaluate(data_tr, data_te):\n",
        "    # Turn on evaluation mode\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    global update_count\n",
        "    e_idxlist = list(range(data_tr.shape[0]))\n",
        "    e_N = data_tr.shape[0]\n",
        "    n100_list = []\n",
        "    r20_list = []\n",
        "    r50_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for start_idx in range(0, e_N, batch_size):\n",
        "            end_idx = min(start_idx + batch_size, N)\n",
        "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
        "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
        "\n",
        "            data_tensor = naive_sparse2tensor(data).to(device)\n",
        "\n",
        "            if total_anneal_steps > 0:\n",
        "                anneal = min(anneal_cap, \n",
        "                               1. * update_count / total_anneal_steps)\n",
        "            else:\n",
        "                anneal = anneal_cap\n",
        "\n",
        "            recon_batch, mu, logvar = model(data_tensor)\n",
        "\n",
        "            loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Exclude examples from training set\n",
        "            recon_batch = recon_batch.cpu().numpy()\n",
        "            recon_batch[data.nonzero()] = -np.inf\n",
        "\n",
        "            n100 = NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
        "            r20 = Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
        "            r50 = Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
        "\n",
        "            n100_list.append(n100)\n",
        "            r20_list.append(r20)\n",
        "            r50_list.append(r50)\n",
        " \n",
        "    total_loss /= len(range(0, e_N, batch_size))\n",
        "    n100_list = np.concatenate(n100_list)\n",
        "    r20_list = np.concatenate(r20_list)\n",
        "    r50_list = np.concatenate(r50_list)\n",
        "\n",
        "    return total_loss, np.mean(n100_list), np.mean(r20_list), np.mean(r50_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33BHkOMd9wA6",
        "outputId": "a7576d44-2f56-4c3c-a49c-0469965433aa"
      },
      "source": [
        "best_n100 = -np.inf\n",
        "update_count = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss, n100, r20, r50 = evaluate(vad_data_tr, vad_data_te)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
        "            'n100 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
        "                epoch, time.time() - epoch_start_time, val_loss,\n",
        "                n100, r20, r50))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if n100 > best_n100:\n",
        "        with open(DATA_DIR + 'model.pt', 'wb') as f:\n",
        "            torch.save(model, f)\n",
        "        best_n100 = n100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |  100/ 233 batches | ms/batch 91.91 | loss 574.67\n",
            "| epoch   1 |  200/ 233 batches | ms/batch 87.49 | loss 533.67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 27.91s | valid loss 418.46 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |  100/ 233 batches | ms/batch 86.06 | loss 521.42\n",
            "| epoch   2 |  200/ 233 batches | ms/batch 85.37 | loss 503.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 26.89s | valid loss 402.67 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |  100/ 233 batches | ms/batch 85.41 | loss 500.98\n",
            "| epoch   3 |  200/ 233 batches | ms/batch 84.40 | loss 491.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 26.92s | valid loss 390.94 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |  100/ 233 batches | ms/batch 85.65 | loss 492.12\n",
            "| epoch   4 |  200/ 233 batches | ms/batch 84.82 | loss 482.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 26.82s | valid loss 385.70 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |  100/ 233 batches | ms/batch 89.03 | loss 482.68\n",
            "| epoch   5 |  200/ 233 batches | ms/batch 87.84 | loss 480.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 27.70s | valid loss 381.51 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |  100/ 233 batches | ms/batch 90.07 | loss 481.70\n",
            "| epoch   6 |  200/ 233 batches | ms/batch 88.07 | loss 475.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 27.70s | valid loss 378.62 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |  100/ 233 batches | ms/batch 88.76 | loss 475.04\n",
            "| epoch   7 |  200/ 233 batches | ms/batch 87.80 | loss 473.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 27.60s | valid loss 376.57 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |  100/ 233 batches | ms/batch 85.99 | loss 477.26\n",
            "| epoch   8 |  200/ 233 batches | ms/batch 84.37 | loss 468.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 26.95s | valid loss 374.72 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |  100/ 233 batches | ms/batch 88.90 | loss 472.55\n",
            "| epoch   9 |  200/ 233 batches | ms/batch 88.16 | loss 469.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 27.63s | valid loss 372.97 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |  100/ 233 batches | ms/batch 89.75 | loss 469.10\n",
            "| epoch  10 |  200/ 233 batches | ms/batch 88.06 | loss 466.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 27.67s | valid loss 371.64 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |  100/ 233 batches | ms/batch 85.21 | loss 469.91\n",
            "| epoch  11 |  200/ 233 batches | ms/batch 84.56 | loss 467.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 26.95s | valid loss 370.20 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |  100/ 233 batches | ms/batch 85.83 | loss 468.92\n",
            "| epoch  12 |  200/ 233 batches | ms/batch 84.17 | loss 463.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 26.87s | valid loss 369.16 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |  100/ 233 batches | ms/batch 84.83 | loss 468.06\n",
            "| epoch  13 |  200/ 233 batches | ms/batch 83.79 | loss 462.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 26.73s | valid loss 368.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |  100/ 233 batches | ms/batch 85.72 | loss 467.35\n",
            "| epoch  14 |  200/ 233 batches | ms/batch 84.43 | loss 463.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 26.97s | valid loss 367.03 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |  100/ 233 batches | ms/batch 85.17 | loss 468.25\n",
            "| epoch  15 |  200/ 233 batches | ms/batch 84.55 | loss 461.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 27.01s | valid loss 366.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |  100/ 233 batches | ms/batch 85.94 | loss 460.57\n",
            "| epoch  16 |  200/ 233 batches | ms/batch 84.13 | loss 461.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 26.80s | valid loss 365.57 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |  100/ 233 batches | ms/batch 84.92 | loss 462.64\n",
            "| epoch  17 |  200/ 233 batches | ms/batch 84.39 | loss 463.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 26.85s | valid loss 364.78 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |  100/ 233 batches | ms/batch 85.98 | loss 458.11\n",
            "| epoch  18 |  200/ 233 batches | ms/batch 84.34 | loss 462.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 26.86s | valid loss 364.26 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |  100/ 233 batches | ms/batch 85.43 | loss 465.51\n",
            "| epoch  19 |  200/ 233 batches | ms/batch 84.55 | loss 456.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 27.03s | valid loss 363.78 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |  100/ 233 batches | ms/batch 86.22 | loss 458.69\n",
            "| epoch  20 |  200/ 233 batches | ms/batch 84.12 | loss 460.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 26.92s | valid loss 363.28 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |  100/ 233 batches | ms/batch 85.27 | loss 459.81\n",
            "| epoch  21 |  200/ 233 batches | ms/batch 84.31 | loss 456.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 26.86s | valid loss 362.82 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |  100/ 233 batches | ms/batch 86.00 | loss 460.99\n",
            "| epoch  22 |  200/ 233 batches | ms/batch 84.59 | loss 458.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 26.99s | valid loss 362.45 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |  100/ 233 batches | ms/batch 85.44 | loss 457.47\n",
            "| epoch  23 |  200/ 233 batches | ms/batch 84.33 | loss 456.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 26.98s | valid loss 362.11 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |  100/ 233 batches | ms/batch 86.38 | loss 462.75\n",
            "| epoch  24 |  200/ 233 batches | ms/batch 84.21 | loss 452.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 27.04s | valid loss 361.73 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |  100/ 233 batches | ms/batch 84.96 | loss 462.16\n",
            "| epoch  25 |  200/ 233 batches | ms/batch 84.12 | loss 452.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 26.86s | valid loss 361.54 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |  100/ 233 batches | ms/batch 85.64 | loss 461.25\n",
            "| epoch  26 |  200/ 233 batches | ms/batch 83.82 | loss 451.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 26.82s | valid loss 361.25 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |  100/ 233 batches | ms/batch 84.84 | loss 453.34\n",
            "| epoch  27 |  200/ 233 batches | ms/batch 83.96 | loss 455.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 26.88s | valid loss 361.09 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |  100/ 233 batches | ms/batch 85.80 | loss 459.39\n",
            "| epoch  28 |  200/ 233 batches | ms/batch 84.28 | loss 456.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 26.83s | valid loss 361.10 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |  100/ 233 batches | ms/batch 84.74 | loss 458.79\n",
            "| epoch  29 |  200/ 233 batches | ms/batch 84.15 | loss 457.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 26.86s | valid loss 360.74 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |  100/ 233 batches | ms/batch 86.21 | loss 456.83\n",
            "| epoch  30 |  200/ 233 batches | ms/batch 84.00 | loss 455.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 26.93s | valid loss 360.70 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |  100/ 233 batches | ms/batch 88.73 | loss 459.89\n",
            "| epoch  31 |  200/ 233 batches | ms/batch 87.69 | loss 454.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 27.64s | valid loss 360.45 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |  100/ 233 batches | ms/batch 89.31 | loss 459.04\n",
            "| epoch  32 |  200/ 233 batches | ms/batch 87.69 | loss 453.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 27.57s | valid loss 360.24 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |  100/ 233 batches | ms/batch 88.70 | loss 452.84\n",
            "| epoch  33 |  200/ 233 batches | ms/batch 87.72 | loss 457.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 27.61s | valid loss 360.27 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |  100/ 233 batches | ms/batch 89.44 | loss 451.58\n",
            "| epoch  34 |  200/ 233 batches | ms/batch 87.90 | loss 457.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 27.65s | valid loss 360.43 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |  100/ 233 batches | ms/batch 88.54 | loss 456.26\n",
            "| epoch  35 |  200/ 233 batches | ms/batch 87.61 | loss 454.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 27.56s | valid loss 360.24 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |  100/ 233 batches | ms/batch 89.64 | loss 461.10\n",
            "| epoch  36 |  200/ 233 batches | ms/batch 87.68 | loss 447.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 27.72s | valid loss 360.09 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |  100/ 233 batches | ms/batch 88.60 | loss 460.69\n",
            "| epoch  37 |  200/ 233 batches | ms/batch 87.18 | loss 450.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 27.54s | valid loss 360.20 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |  100/ 233 batches | ms/batch 89.42 | loss 456.17\n",
            "| epoch  38 |  200/ 233 batches | ms/batch 87.26 | loss 455.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 27.56s | valid loss 360.18 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |  100/ 233 batches | ms/batch 88.84 | loss 462.73\n",
            "| epoch  39 |  200/ 233 batches | ms/batch 87.77 | loss 445.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 27.71s | valid loss 360.15 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |  100/ 233 batches | ms/batch 89.78 | loss 457.01\n",
            "| epoch  40 |  200/ 233 batches | ms/batch 88.35 | loss 452.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 27.66s | valid loss 360.25 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |  100/ 233 batches | ms/batch 88.97 | loss 457.66\n",
            "| epoch  41 |  200/ 233 batches | ms/batch 87.44 | loss 452.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 27.62s | valid loss 360.30 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |  100/ 233 batches | ms/batch 89.84 | loss 457.92\n",
            "| epoch  42 |  200/ 233 batches | ms/batch 88.02 | loss 452.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 27.65s | valid loss 360.35 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |  100/ 233 batches | ms/batch 89.14 | loss 457.70\n",
            "| epoch  43 |  200/ 233 batches | ms/batch 87.95 | loss 452.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 27.64s | valid loss 360.37 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |  100/ 233 batches | ms/batch 89.66 | loss 456.57\n",
            "| epoch  44 |  200/ 233 batches | ms/batch 88.13 | loss 454.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 27.73s | valid loss 360.27 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |  100/ 233 batches | ms/batch 89.10 | loss 458.93\n",
            "| epoch  45 |  200/ 233 batches | ms/batch 88.06 | loss 449.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 27.76s | valid loss 360.27 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |  100/ 233 batches | ms/batch 89.47 | loss 455.54\n",
            "| epoch  46 |  200/ 233 batches | ms/batch 88.12 | loss 455.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 27.72s | valid loss 360.21 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |  100/ 233 batches | ms/batch 89.24 | loss 456.93\n",
            "| epoch  47 |  200/ 233 batches | ms/batch 87.93 | loss 451.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 27.78s | valid loss 360.52 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |  100/ 233 batches | ms/batch 89.70 | loss 454.95\n",
            "| epoch  48 |  200/ 233 batches | ms/batch 87.90 | loss 455.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 27.72s | valid loss 360.43 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |  100/ 233 batches | ms/batch 89.09 | loss 455.74\n",
            "| epoch  49 |  200/ 233 batches | ms/batch 88.05 | loss 451.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 27.76s | valid loss 360.56 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |  100/ 233 batches | ms/batch 89.90 | loss 457.41\n",
            "| epoch  50 |  200/ 233 batches | ms/batch 87.77 | loss 450.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 27.61s | valid loss 360.63 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  51 |  100/ 233 batches | ms/batch 88.39 | loss 454.05\n",
            "| epoch  51 |  200/ 233 batches | ms/batch 88.00 | loss 455.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 27.54s | valid loss 360.68 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |  100/ 233 batches | ms/batch 89.35 | loss 455.20\n",
            "| epoch  52 |  200/ 233 batches | ms/batch 87.87 | loss 455.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 27.53s | valid loss 360.82 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  53 |  100/ 233 batches | ms/batch 88.77 | loss 460.77\n",
            "| epoch  53 |  200/ 233 batches | ms/batch 87.97 | loss 449.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 27.65s | valid loss 360.73 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |  100/ 233 batches | ms/batch 89.52 | loss 457.97\n",
            "| epoch  54 |  200/ 233 batches | ms/batch 87.75 | loss 453.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 27.68s | valid loss 360.76 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  55 |  100/ 233 batches | ms/batch 89.13 | loss 453.22\n",
            "| epoch  55 |  200/ 233 batches | ms/batch 87.75 | loss 456.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 27.58s | valid loss 360.99 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  56 |  100/ 233 batches | ms/batch 91.23 | loss 453.17\n",
            "| epoch  56 |  200/ 233 batches | ms/batch 87.87 | loss 454.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 27.84s | valid loss 360.98 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  57 |  100/ 233 batches | ms/batch 88.99 | loss 461.59\n",
            "| epoch  57 |  200/ 233 batches | ms/batch 87.77 | loss 451.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 27.70s | valid loss 361.08 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  58 |  100/ 233 batches | ms/batch 88.52 | loss 459.09\n",
            "| epoch  58 |  200/ 233 batches | ms/batch 87.89 | loss 451.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 27.56s | valid loss 361.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  59 |  100/ 233 batches | ms/batch 88.23 | loss 459.84\n",
            "| epoch  59 |  200/ 233 batches | ms/batch 87.29 | loss 453.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 27.51s | valid loss 361.15 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  60 |  100/ 233 batches | ms/batch 89.23 | loss 460.49\n",
            "| epoch  60 |  200/ 233 batches | ms/batch 88.01 | loss 450.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 27.59s | valid loss 361.29 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  61 |  100/ 233 batches | ms/batch 88.76 | loss 456.02\n",
            "| epoch  61 |  200/ 233 batches | ms/batch 87.45 | loss 456.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 27.55s | valid loss 361.41 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  62 |  100/ 233 batches | ms/batch 89.37 | loss 457.32\n",
            "| epoch  62 |  200/ 233 batches | ms/batch 87.87 | loss 454.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 27.48s | valid loss 361.33 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  63 |  100/ 233 batches | ms/batch 88.60 | loss 459.34\n",
            "| epoch  63 |  200/ 233 batches | ms/batch 87.55 | loss 451.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 27.48s | valid loss 361.35 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  64 |  100/ 233 batches | ms/batch 89.68 | loss 453.07\n",
            "| epoch  64 |  200/ 233 batches | ms/batch 87.65 | loss 456.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 27.45s | valid loss 361.67 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  65 |  100/ 233 batches | ms/batch 88.74 | loss 455.62\n",
            "| epoch  65 |  200/ 233 batches | ms/batch 87.89 | loss 453.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 27.51s | valid loss 361.60 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  66 |  100/ 233 batches | ms/batch 89.57 | loss 455.82\n",
            "| epoch  66 |  200/ 233 batches | ms/batch 87.77 | loss 454.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 27.45s | valid loss 361.72 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  67 |  100/ 233 batches | ms/batch 88.55 | loss 461.03\n",
            "| epoch  67 |  200/ 233 batches | ms/batch 87.53 | loss 451.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 27.45s | valid loss 361.81 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  68 |  100/ 233 batches | ms/batch 89.48 | loss 454.89\n",
            "| epoch  68 |  200/ 233 batches | ms/batch 87.65 | loss 455.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 27.46s | valid loss 362.11 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  69 |  100/ 233 batches | ms/batch 88.50 | loss 454.23\n",
            "| epoch  69 |  200/ 233 batches | ms/batch 88.03 | loss 457.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 27.53s | valid loss 361.99 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  70 |  100/ 233 batches | ms/batch 89.12 | loss 453.29\n",
            "| epoch  70 |  200/ 233 batches | ms/batch 87.26 | loss 454.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 27.35s | valid loss 361.96 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  71 |  100/ 233 batches | ms/batch 88.88 | loss 452.06\n",
            "| epoch  71 |  200/ 233 batches | ms/batch 88.01 | loss 457.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 27.58s | valid loss 362.13 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  72 |  100/ 233 batches | ms/batch 89.56 | loss 461.29\n",
            "| epoch  72 |  200/ 233 batches | ms/batch 87.67 | loss 451.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 27.48s | valid loss 362.06 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  73 |  100/ 233 batches | ms/batch 86.40 | loss 457.91\n",
            "| epoch  73 |  200/ 233 batches | ms/batch 85.22 | loss 457.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 26.83s | valid loss 362.29 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  74 |  100/ 233 batches | ms/batch 86.89 | loss 458.47\n",
            "| epoch  74 |  200/ 233 batches | ms/batch 84.62 | loss 452.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 26.59s | valid loss 362.33 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  75 |  100/ 233 batches | ms/batch 85.96 | loss 462.54\n",
            "| epoch  75 |  200/ 233 batches | ms/batch 84.65 | loss 450.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 26.63s | valid loss 362.31 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  76 |  100/ 233 batches | ms/batch 86.48 | loss 459.86\n",
            "| epoch  76 |  200/ 233 batches | ms/batch 84.75 | loss 454.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 26.57s | valid loss 362.46 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  77 |  100/ 233 batches | ms/batch 85.11 | loss 461.20\n",
            "| epoch  77 |  200/ 233 batches | ms/batch 84.59 | loss 449.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 26.48s | valid loss 362.59 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  78 |  100/ 233 batches | ms/batch 86.21 | loss 457.54\n",
            "| epoch  78 |  200/ 233 batches | ms/batch 84.96 | loss 455.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 26.65s | valid loss 362.54 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  79 |  100/ 233 batches | ms/batch 85.53 | loss 459.12\n",
            "| epoch  79 |  200/ 233 batches | ms/batch 84.84 | loss 456.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 26.72s | valid loss 362.74 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  80 |  100/ 233 batches | ms/batch 86.47 | loss 465.23\n",
            "| epoch  80 |  200/ 233 batches | ms/batch 85.25 | loss 449.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 26.64s | valid loss 362.63 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  81 |  100/ 233 batches | ms/batch 85.53 | loss 454.51\n",
            "| epoch  81 |  200/ 233 batches | ms/batch 84.87 | loss 458.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 26.64s | valid loss 362.95 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  82 |  100/ 233 batches | ms/batch 89.05 | loss 461.05\n",
            "| epoch  82 |  200/ 233 batches | ms/batch 84.18 | loss 451.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 26.96s | valid loss 363.01 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  83 |  100/ 233 batches | ms/batch 85.96 | loss 461.10\n",
            "| epoch  83 |  200/ 233 batches | ms/batch 84.25 | loss 452.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 26.53s | valid loss 363.14 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  84 |  100/ 233 batches | ms/batch 85.69 | loss 460.11\n",
            "| epoch  84 |  200/ 233 batches | ms/batch 84.60 | loss 454.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 26.62s | valid loss 363.21 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  85 |  100/ 233 batches | ms/batch 85.19 | loss 457.87\n",
            "| epoch  85 |  200/ 233 batches | ms/batch 84.13 | loss 455.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 26.44s | valid loss 363.16 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  86 |  100/ 233 batches | ms/batch 88.62 | loss 459.02\n",
            "| epoch  86 |  200/ 233 batches | ms/batch 83.34 | loss 456.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 26.59s | valid loss 363.37 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  87 |  100/ 233 batches | ms/batch 85.24 | loss 458.84\n",
            "| epoch  87 |  200/ 233 batches | ms/batch 83.99 | loss 455.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 26.47s | valid loss 363.41 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  88 |  100/ 233 batches | ms/batch 84.52 | loss 458.52\n",
            "| epoch  88 |  200/ 233 batches | ms/batch 84.03 | loss 456.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 26.35s | valid loss 363.39 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  89 |  100/ 233 batches | ms/batch 84.73 | loss 458.62\n",
            "| epoch  89 |  200/ 233 batches | ms/batch 84.60 | loss 456.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 26.60s | valid loss 363.42 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  90 |  100/ 233 batches | ms/batch 85.67 | loss 454.27\n",
            "| epoch  90 |  200/ 233 batches | ms/batch 84.31 | loss 457.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 26.58s | valid loss 363.78 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  91 |  100/ 233 batches | ms/batch 84.72 | loss 462.11\n",
            "| epoch  91 |  200/ 233 batches | ms/batch 84.25 | loss 452.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 26.52s | valid loss 363.70 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  92 |  100/ 233 batches | ms/batch 85.38 | loss 462.08\n",
            "| epoch  92 |  200/ 233 batches | ms/batch 83.99 | loss 451.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 26.42s | valid loss 363.61 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  93 |  100/ 233 batches | ms/batch 84.65 | loss 464.34\n",
            "| epoch  93 |  200/ 233 batches | ms/batch 84.12 | loss 454.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time: 26.75s | valid loss 363.88 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  94 |  100/ 233 batches | ms/batch 85.64 | loss 455.17\n",
            "| epoch  94 |  200/ 233 batches | ms/batch 84.12 | loss 459.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time: 26.53s | valid loss 363.92 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  95 |  100/ 233 batches | ms/batch 84.77 | loss 456.71\n",
            "| epoch  95 |  200/ 233 batches | ms/batch 83.44 | loss 458.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time: 26.30s | valid loss 364.10 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  96 |  100/ 233 batches | ms/batch 85.15 | loss 459.77\n",
            "| epoch  96 |  200/ 233 batches | ms/batch 83.65 | loss 456.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time: 26.32s | valid loss 364.16 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  97 |  100/ 233 batches | ms/batch 84.32 | loss 455.94\n",
            "| epoch  97 |  200/ 233 batches | ms/batch 83.57 | loss 459.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time: 26.40s | valid loss 364.15 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  98 |  100/ 233 batches | ms/batch 85.51 | loss 461.50\n",
            "| epoch  98 |  200/ 233 batches | ms/batch 83.76 | loss 453.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time: 26.51s | valid loss 364.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  99 |  100/ 233 batches | ms/batch 84.93 | loss 456.50\n",
            "| epoch  99 |  200/ 233 batches | ms/batch 84.19 | loss 453.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time: 26.53s | valid loss 364.27 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 100 |  100/ 233 batches | ms/batch 87.13 | loss 460.16\n",
            "| epoch 100 |  200/ 233 batches | ms/batch 85.59 | loss 455.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time: 26.98s | valid loss 364.28 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 101 |  100/ 233 batches | ms/batch 86.58 | loss 461.21\n",
            "| epoch 101 |  200/ 233 batches | ms/batch 85.96 | loss 453.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 101 | time: 27.01s | valid loss 364.39 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 102 |  100/ 233 batches | ms/batch 87.36 | loss 459.19\n",
            "| epoch 102 |  200/ 233 batches | ms/batch 85.78 | loss 456.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 102 | time: 27.09s | valid loss 364.37 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 103 |  100/ 233 batches | ms/batch 87.01 | loss 458.42\n",
            "| epoch 103 |  200/ 233 batches | ms/batch 86.12 | loss 457.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 103 | time: 27.16s | valid loss 364.69 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 104 |  100/ 233 batches | ms/batch 87.57 | loss 459.76\n",
            "| epoch 104 |  200/ 233 batches | ms/batch 86.11 | loss 457.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 104 | time: 27.10s | valid loss 364.63 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 105 |  100/ 233 batches | ms/batch 86.66 | loss 461.61\n",
            "| epoch 105 |  200/ 233 batches | ms/batch 85.29 | loss 453.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 105 | time: 26.84s | valid loss 364.56 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 106 |  100/ 233 batches | ms/batch 86.93 | loss 462.48\n",
            "| epoch 106 |  200/ 233 batches | ms/batch 85.33 | loss 456.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 106 | time: 26.81s | valid loss 364.81 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 107 |  100/ 233 batches | ms/batch 85.94 | loss 460.98\n",
            "| epoch 107 |  200/ 233 batches | ms/batch 85.50 | loss 454.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 107 | time: 27.51s | valid loss 364.70 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 108 |  100/ 233 batches | ms/batch 84.38 | loss 457.35\n",
            "| epoch 108 |  200/ 233 batches | ms/batch 83.27 | loss 458.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 108 | time: 26.16s | valid loss 364.90 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 109 |  100/ 233 batches | ms/batch 84.32 | loss 455.80\n",
            "| epoch 109 |  200/ 233 batches | ms/batch 84.29 | loss 458.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 109 | time: 26.40s | valid loss 365.00 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 110 |  100/ 233 batches | ms/batch 85.53 | loss 462.03\n",
            "| epoch 110 |  200/ 233 batches | ms/batch 84.07 | loss 456.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 110 | time: 26.52s | valid loss 365.05 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 111 |  100/ 233 batches | ms/batch 85.43 | loss 458.25\n",
            "| epoch 111 |  200/ 233 batches | ms/batch 84.60 | loss 455.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 111 | time: 26.58s | valid loss 364.96 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 112 |  100/ 233 batches | ms/batch 86.34 | loss 460.97\n",
            "| epoch 112 |  200/ 233 batches | ms/batch 84.51 | loss 454.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 112 | time: 26.69s | valid loss 365.13 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 113 |  100/ 233 batches | ms/batch 85.57 | loss 459.88\n",
            "| epoch 113 |  200/ 233 batches | ms/batch 84.67 | loss 456.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 113 | time: 26.68s | valid loss 365.39 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 114 |  100/ 233 batches | ms/batch 86.70 | loss 454.14\n",
            "| epoch 114 |  200/ 233 batches | ms/batch 85.19 | loss 462.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 114 | time: 26.86s | valid loss 365.42 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 115 |  100/ 233 batches | ms/batch 85.88 | loss 463.62\n",
            "| epoch 115 |  200/ 233 batches | ms/batch 84.86 | loss 454.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 115 | time: 26.74s | valid loss 365.53 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 116 |  100/ 233 batches | ms/batch 86.48 | loss 460.10\n",
            "| epoch 116 |  200/ 233 batches | ms/batch 84.48 | loss 456.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 116 | time: 26.66s | valid loss 365.42 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 117 |  100/ 233 batches | ms/batch 85.58 | loss 464.69\n",
            "| epoch 117 |  200/ 233 batches | ms/batch 85.07 | loss 452.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 117 | time: 26.71s | valid loss 365.39 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 118 |  100/ 233 batches | ms/batch 86.39 | loss 455.37\n",
            "| epoch 118 |  200/ 233 batches | ms/batch 85.24 | loss 463.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 118 | time: 26.71s | valid loss 365.48 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 119 |  100/ 233 batches | ms/batch 85.33 | loss 464.67\n",
            "| epoch 119 |  200/ 233 batches | ms/batch 84.98 | loss 453.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 119 | time: 26.79s | valid loss 365.64 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 120 |  100/ 233 batches | ms/batch 86.75 | loss 459.46\n",
            "| epoch 120 |  200/ 233 batches | ms/batch 86.38 | loss 458.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 120 | time: 27.04s | valid loss 365.67 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 121 |  100/ 233 batches | ms/batch 85.59 | loss 457.11\n",
            "| epoch 121 |  200/ 233 batches | ms/batch 85.73 | loss 459.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 121 | time: 26.87s | valid loss 365.83 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 122 |  100/ 233 batches | ms/batch 86.81 | loss 461.03\n",
            "| epoch 122 |  200/ 233 batches | ms/batch 84.76 | loss 459.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 122 | time: 26.74s | valid loss 365.92 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 123 |  100/ 233 batches | ms/batch 85.62 | loss 460.97\n",
            "| epoch 123 |  200/ 233 batches | ms/batch 84.02 | loss 458.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 123 | time: 26.52s | valid loss 365.85 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 124 |  100/ 233 batches | ms/batch 85.45 | loss 464.31\n",
            "| epoch 124 |  200/ 233 batches | ms/batch 83.31 | loss 456.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 124 | time: 26.26s | valid loss 366.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 125 |  100/ 233 batches | ms/batch 84.40 | loss 463.26\n",
            "| epoch 125 |  200/ 233 batches | ms/batch 83.80 | loss 456.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 125 | time: 26.20s | valid loss 366.17 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 126 |  100/ 233 batches | ms/batch 85.35 | loss 460.90\n",
            "| epoch 126 |  200/ 233 batches | ms/batch 83.70 | loss 459.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 126 | time: 26.26s | valid loss 366.13 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 127 |  100/ 233 batches | ms/batch 84.37 | loss 462.60\n",
            "| epoch 127 |  200/ 233 batches | ms/batch 83.57 | loss 455.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 127 | time: 26.18s | valid loss 366.22 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 128 |  100/ 233 batches | ms/batch 85.02 | loss 463.08\n",
            "| epoch 128 |  200/ 233 batches | ms/batch 83.62 | loss 455.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 128 | time: 26.21s | valid loss 366.30 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 129 |  100/ 233 batches | ms/batch 84.40 | loss 463.85\n",
            "| epoch 129 |  200/ 233 batches | ms/batch 83.52 | loss 457.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 129 | time: 26.22s | valid loss 366.33 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 130 |  100/ 233 batches | ms/batch 84.91 | loss 462.99\n",
            "| epoch 130 |  200/ 233 batches | ms/batch 83.30 | loss 453.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 130 | time: 26.15s | valid loss 366.54 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 131 |  100/ 233 batches | ms/batch 84.36 | loss 459.56\n",
            "| epoch 131 |  200/ 233 batches | ms/batch 83.21 | loss 457.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 131 | time: 26.02s | valid loss 366.58 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 132 |  100/ 233 batches | ms/batch 85.03 | loss 461.46\n",
            "| epoch 132 |  200/ 233 batches | ms/batch 83.71 | loss 457.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 132 | time: 26.13s | valid loss 366.65 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 133 |  100/ 233 batches | ms/batch 84.16 | loss 459.52\n",
            "| epoch 133 |  200/ 233 batches | ms/batch 83.80 | loss 462.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 133 | time: 26.21s | valid loss 366.88 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 134 |  100/ 233 batches | ms/batch 85.51 | loss 461.71\n",
            "| epoch 134 |  200/ 233 batches | ms/batch 83.45 | loss 459.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 134 | time: 26.23s | valid loss 366.91 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 135 |  100/ 233 batches | ms/batch 84.01 | loss 458.61\n",
            "| epoch 135 |  200/ 233 batches | ms/batch 83.67 | loss 458.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 135 | time: 26.08s | valid loss 366.84 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 136 |  100/ 233 batches | ms/batch 85.58 | loss 462.34\n",
            "| epoch 136 |  200/ 233 batches | ms/batch 83.49 | loss 457.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 136 | time: 26.13s | valid loss 366.90 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 137 |  100/ 233 batches | ms/batch 84.64 | loss 460.70\n",
            "| epoch 137 |  200/ 233 batches | ms/batch 83.53 | loss 460.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 137 | time: 26.16s | valid loss 366.91 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 138 |  100/ 233 batches | ms/batch 85.10 | loss 463.82\n",
            "| epoch 138 |  200/ 233 batches | ms/batch 83.79 | loss 458.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 138 | time: 26.14s | valid loss 366.99 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 139 |  100/ 233 batches | ms/batch 84.26 | loss 462.92\n",
            "| epoch 139 |  200/ 233 batches | ms/batch 83.64 | loss 455.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 139 | time: 26.17s | valid loss 366.96 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 140 |  100/ 233 batches | ms/batch 85.15 | loss 463.39\n",
            "| epoch 140 |  200/ 233 batches | ms/batch 83.43 | loss 458.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 140 | time: 26.12s | valid loss 367.24 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 141 |  100/ 233 batches | ms/batch 84.24 | loss 466.15\n",
            "| epoch 141 |  200/ 233 batches | ms/batch 83.65 | loss 453.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 141 | time: 26.15s | valid loss 367.16 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 142 |  100/ 233 batches | ms/batch 85.17 | loss 464.93\n",
            "| epoch 142 |  200/ 233 batches | ms/batch 83.76 | loss 454.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 142 | time: 26.16s | valid loss 367.25 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 143 |  100/ 233 batches | ms/batch 84.43 | loss 464.52\n",
            "| epoch 143 |  200/ 233 batches | ms/batch 83.32 | loss 458.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 143 | time: 26.25s | valid loss 367.32 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 144 |  100/ 233 batches | ms/batch 85.43 | loss 461.36\n",
            "| epoch 144 |  200/ 233 batches | ms/batch 83.53 | loss 459.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 144 | time: 26.34s | valid loss 367.38 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 145 |  100/ 233 batches | ms/batch 84.28 | loss 464.22\n",
            "| epoch 145 |  200/ 233 batches | ms/batch 83.37 | loss 458.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 145 | time: 26.22s | valid loss 367.52 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 146 |  100/ 233 batches | ms/batch 85.19 | loss 463.85\n",
            "| epoch 146 |  200/ 233 batches | ms/batch 83.48 | loss 460.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 146 | time: 26.25s | valid loss 367.63 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 147 |  100/ 233 batches | ms/batch 84.73 | loss 461.43\n",
            "| epoch 147 |  200/ 233 batches | ms/batch 83.38 | loss 461.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 147 | time: 26.13s | valid loss 367.56 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 148 |  100/ 233 batches | ms/batch 84.83 | loss 463.10\n",
            "| epoch 148 |  200/ 233 batches | ms/batch 83.35 | loss 458.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 148 | time: 26.25s | valid loss 367.63 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 149 |  100/ 233 batches | ms/batch 85.52 | loss 462.75\n",
            "| epoch 149 |  200/ 233 batches | ms/batch 83.28 | loss 458.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 149 | time: 26.30s | valid loss 367.55 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 150 |  100/ 233 batches | ms/batch 85.32 | loss 464.84\n",
            "| epoch 150 |  200/ 233 batches | ms/batch 83.61 | loss 456.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 150 | time: 26.31s | valid loss 367.77 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 151 |  100/ 233 batches | ms/batch 84.40 | loss 458.07\n",
            "| epoch 151 |  200/ 233 batches | ms/batch 83.35 | loss 462.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 151 | time: 26.18s | valid loss 367.98 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 152 |  100/ 233 batches | ms/batch 84.63 | loss 464.58\n",
            "| epoch 152 |  200/ 233 batches | ms/batch 83.43 | loss 457.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 152 | time: 26.16s | valid loss 367.74 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 153 |  100/ 233 batches | ms/batch 84.40 | loss 462.08\n",
            "| epoch 153 |  200/ 233 batches | ms/batch 83.31 | loss 460.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 153 | time: 26.23s | valid loss 367.91 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 154 |  100/ 233 batches | ms/batch 85.28 | loss 465.04\n",
            "| epoch 154 |  200/ 233 batches | ms/batch 83.63 | loss 458.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 154 | time: 26.12s | valid loss 367.99 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 155 |  100/ 233 batches | ms/batch 84.36 | loss 461.50\n",
            "| epoch 155 |  200/ 233 batches | ms/batch 83.68 | loss 460.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 155 | time: 26.25s | valid loss 368.04 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 156 |  100/ 233 batches | ms/batch 84.76 | loss 464.06\n",
            "| epoch 156 |  200/ 233 batches | ms/batch 84.00 | loss 459.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 156 | time: 26.23s | valid loss 368.24 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 157 |  100/ 233 batches | ms/batch 84.73 | loss 468.77\n",
            "| epoch 157 |  200/ 233 batches | ms/batch 83.66 | loss 454.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 157 | time: 26.31s | valid loss 368.32 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 158 |  100/ 233 batches | ms/batch 85.21 | loss 462.79\n",
            "| epoch 158 |  200/ 233 batches | ms/batch 83.44 | loss 461.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 158 | time: 26.17s | valid loss 368.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 159 |  100/ 233 batches | ms/batch 84.84 | loss 465.69\n",
            "| epoch 159 |  200/ 233 batches | ms/batch 83.87 | loss 459.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 159 | time: 26.30s | valid loss 368.54 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 160 |  100/ 233 batches | ms/batch 85.22 | loss 463.26\n",
            "| epoch 160 |  200/ 233 batches | ms/batch 83.46 | loss 458.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 160 | time: 26.18s | valid loss 368.45 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 161 |  100/ 233 batches | ms/batch 84.64 | loss 460.05\n",
            "| epoch 161 |  200/ 233 batches | ms/batch 83.83 | loss 462.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 161 | time: 26.25s | valid loss 368.52 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 162 |  100/ 233 batches | ms/batch 85.45 | loss 463.16\n",
            "| epoch 162 |  200/ 233 batches | ms/batch 83.16 | loss 460.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 162 | time: 26.22s | valid loss 368.61 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 163 |  100/ 233 batches | ms/batch 84.60 | loss 465.83\n",
            "| epoch 163 |  200/ 233 batches | ms/batch 83.68 | loss 456.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 163 | time: 26.20s | valid loss 368.60 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 164 |  100/ 233 batches | ms/batch 84.90 | loss 467.70\n",
            "| epoch 164 |  200/ 233 batches | ms/batch 83.44 | loss 457.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 164 | time: 26.68s | valid loss 368.74 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 165 |  100/ 233 batches | ms/batch 90.36 | loss 462.72\n",
            "| epoch 165 |  200/ 233 batches | ms/batch 83.08 | loss 462.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 165 | time: 26.80s | valid loss 368.67 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 166 |  100/ 233 batches | ms/batch 84.20 | loss 461.63\n",
            "| epoch 166 |  200/ 233 batches | ms/batch 82.93 | loss 461.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 166 | time: 26.22s | valid loss 368.76 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 167 |  100/ 233 batches | ms/batch 83.95 | loss 465.76\n",
            "| epoch 167 |  200/ 233 batches | ms/batch 83.46 | loss 459.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 167 | time: 26.44s | valid loss 368.74 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 168 |  100/ 233 batches | ms/batch 85.82 | loss 466.08\n",
            "| epoch 168 |  200/ 233 batches | ms/batch 82.96 | loss 456.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 168 | time: 26.24s | valid loss 368.99 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 169 |  100/ 233 batches | ms/batch 83.08 | loss 464.40\n",
            "| epoch 169 |  200/ 233 batches | ms/batch 82.13 | loss 459.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 169 | time: 25.92s | valid loss 368.83 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 170 |  100/ 233 batches | ms/batch 83.82 | loss 463.39\n",
            "| epoch 170 |  200/ 233 batches | ms/batch 82.11 | loss 462.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 170 | time: 25.79s | valid loss 368.99 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 171 |  100/ 233 batches | ms/batch 82.65 | loss 460.82\n",
            "| epoch 171 |  200/ 233 batches | ms/batch 82.05 | loss 464.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 171 | time: 25.87s | valid loss 368.98 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 172 |  100/ 233 batches | ms/batch 82.92 | loss 465.26\n",
            "| epoch 172 |  200/ 233 batches | ms/batch 81.35 | loss 460.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 172 | time: 25.73s | valid loss 369.09 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 173 |  100/ 233 batches | ms/batch 82.02 | loss 463.49\n",
            "| epoch 173 |  200/ 233 batches | ms/batch 81.32 | loss 459.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 173 | time: 25.70s | valid loss 369.27 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 174 |  100/ 233 batches | ms/batch 83.17 | loss 467.58\n",
            "| epoch 174 |  200/ 233 batches | ms/batch 81.35 | loss 456.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 174 | time: 26.25s | valid loss 369.09 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 175 |  100/ 233 batches | ms/batch 82.29 | loss 461.21\n",
            "| epoch 175 |  200/ 233 batches | ms/batch 81.62 | loss 464.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 175 | time: 25.80s | valid loss 369.13 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 176 |  100/ 233 batches | ms/batch 83.60 | loss 468.56\n",
            "| epoch 176 |  200/ 233 batches | ms/batch 81.59 | loss 456.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 176 | time: 25.77s | valid loss 369.06 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 177 |  100/ 233 batches | ms/batch 82.62 | loss 463.05\n",
            "| epoch 177 |  200/ 233 batches | ms/batch 81.68 | loss 461.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 177 | time: 25.80s | valid loss 369.11 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 178 |  100/ 233 batches | ms/batch 82.92 | loss 466.32\n",
            "| epoch 178 |  200/ 233 batches | ms/batch 81.40 | loss 458.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 178 | time: 25.72s | valid loss 369.45 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 179 |  100/ 233 batches | ms/batch 82.27 | loss 470.86\n",
            "| epoch 179 |  200/ 233 batches | ms/batch 82.92 | loss 451.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 179 | time: 26.09s | valid loss 369.09 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 180 |  100/ 233 batches | ms/batch 84.37 | loss 467.55\n",
            "| epoch 180 |  200/ 233 batches | ms/batch 83.00 | loss 458.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 180 | time: 26.14s | valid loss 369.29 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 181 |  100/ 233 batches | ms/batch 83.82 | loss 468.47\n",
            "| epoch 181 |  200/ 233 batches | ms/batch 82.07 | loss 454.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 181 | time: 26.07s | valid loss 369.08 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 182 |  100/ 233 batches | ms/batch 84.96 | loss 463.02\n",
            "| epoch 182 |  200/ 233 batches | ms/batch 82.76 | loss 459.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 182 | time: 26.27s | valid loss 369.31 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 183 |  100/ 233 batches | ms/batch 83.63 | loss 465.05\n",
            "| epoch 183 |  200/ 233 batches | ms/batch 82.61 | loss 461.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 183 | time: 26.14s | valid loss 369.44 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 184 |  100/ 233 batches | ms/batch 83.92 | loss 462.26\n",
            "| epoch 184 |  200/ 233 batches | ms/batch 82.54 | loss 461.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 184 | time: 25.97s | valid loss 369.06 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 185 |  100/ 233 batches | ms/batch 83.27 | loss 465.80\n",
            "| epoch 185 |  200/ 233 batches | ms/batch 82.02 | loss 456.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 185 | time: 26.09s | valid loss 369.16 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 186 |  100/ 233 batches | ms/batch 84.11 | loss 465.22\n",
            "| epoch 186 |  200/ 233 batches | ms/batch 82.46 | loss 457.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 186 | time: 25.97s | valid loss 369.26 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 187 |  100/ 233 batches | ms/batch 83.43 | loss 462.87\n",
            "| epoch 187 |  200/ 233 batches | ms/batch 82.80 | loss 461.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 187 | time: 26.06s | valid loss 369.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 188 |  100/ 233 batches | ms/batch 84.28 | loss 461.99\n",
            "| epoch 188 |  200/ 233 batches | ms/batch 82.66 | loss 463.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 188 | time: 26.03s | valid loss 369.32 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 189 |  100/ 233 batches | ms/batch 87.82 | loss 464.10\n",
            "| epoch 189 |  200/ 233 batches | ms/batch 81.93 | loss 459.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 189 | time: 26.49s | valid loss 369.16 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 190 |  100/ 233 batches | ms/batch 85.86 | loss 462.08\n",
            "| epoch 190 |  200/ 233 batches | ms/batch 81.54 | loss 462.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 190 | time: 25.99s | valid loss 369.31 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 191 |  100/ 233 batches | ms/batch 82.50 | loss 462.10\n",
            "| epoch 191 |  200/ 233 batches | ms/batch 81.93 | loss 460.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 191 | time: 25.83s | valid loss 369.24 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 192 |  100/ 233 batches | ms/batch 83.81 | loss 464.90\n",
            "| epoch 192 |  200/ 233 batches | ms/batch 82.50 | loss 460.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 192 | time: 25.92s | valid loss 369.23 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 193 |  100/ 233 batches | ms/batch 82.45 | loss 459.64\n",
            "| epoch 193 |  200/ 233 batches | ms/batch 81.80 | loss 463.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 193 | time: 25.87s | valid loss 369.35 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 194 |  100/ 233 batches | ms/batch 83.62 | loss 471.34\n",
            "| epoch 194 |  200/ 233 batches | ms/batch 81.59 | loss 455.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 194 | time: 25.84s | valid loss 369.43 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 195 |  100/ 233 batches | ms/batch 82.54 | loss 468.59\n",
            "| epoch 195 |  200/ 233 batches | ms/batch 81.77 | loss 458.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 195 | time: 25.75s | valid loss 369.38 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 196 |  100/ 233 batches | ms/batch 83.65 | loss 465.94\n",
            "| epoch 196 |  200/ 233 batches | ms/batch 81.89 | loss 455.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 196 | time: 25.71s | valid loss 369.15 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 197 |  100/ 233 batches | ms/batch 82.67 | loss 469.91\n",
            "| epoch 197 |  200/ 233 batches | ms/batch 82.00 | loss 455.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 197 | time: 25.88s | valid loss 369.32 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 198 |  100/ 233 batches | ms/batch 83.71 | loss 466.13\n",
            "| epoch 198 |  200/ 233 batches | ms/batch 82.00 | loss 458.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 198 | time: 26.01s | valid loss 369.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 199 |  100/ 233 batches | ms/batch 83.23 | loss 465.53\n",
            "| epoch 199 |  200/ 233 batches | ms/batch 81.81 | loss 460.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 199 | time: 25.99s | valid loss 369.19 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 200 |  100/ 233 batches | ms/batch 84.04 | loss 459.66\n",
            "| epoch 200 |  200/ 233 batches | ms/batch 82.42 | loss 464.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 200 | time: 26.09s | valid loss 369.34 | n100   nan | r20   nan | r50   nan\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEhWwsl7-RK_",
        "outputId": "2f0cdac9-60ba-42a4-f25b-4133dd78a78e"
      },
      "source": [
        "# with open(DATA_DIR + 'model.pt', 'rb') as f:\n",
        "#     model = torch.load(f)\n",
        "\n",
        "test_loss, n100, r20, r50 = evaluate(test_data_tr, test_data_te)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r20 {:4.2f} | '\n",
        "        'r50 {:4.2f}'.format(test_loss, n100, r20, r50))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss 369.26 | n100  nan | r20  nan | r50  nan\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI4fHMg4TgOB",
        "outputId": "d2880c47-6577-4b70-d784-d1f0b1cf5991"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiVAE(\n",
              "  (q_layers): ModuleList(\n",
              "    (0): Linear(in_features=20101, out_features=600, bias=True)\n",
              "    (1): Linear(in_features=600, out_features=400, bias=True)\n",
              "  )\n",
              "  (p_layers): ModuleList(\n",
              "    (0): Linear(in_features=200, out_features=600, bias=True)\n",
              "    (1): Linear(in_features=600, out_features=20101, bias=True)\n",
              "  )\n",
              "  (drop): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}